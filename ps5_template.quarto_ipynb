{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"title\"\n",
        "author: \"author\"\n",
        "date: \"date\"\n",
        "format: \n",
        "  pdf:\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "output:\n",
        "  echo: false\n",
        "  eval: false\n",
        "---\n",
        "\n",
        "\n",
        "**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**\n",
        "\n",
        "## Submission Steps (10 pts)\n",
        "1. This problem set is a paired problem set.\n",
        "2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.\n",
        "    - Partner 1 (name and cnet ID): Guillermina Marto - gmarto\n",
        "    - Partner 2 (name and cnet ID): Alejandra Silva - aosilva\n",
        "3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. \n",
        "4. \"This submission is our work alone and complies with the 30538 integrity policy.\" Add your initials to indicate your agreement: \\*\\*GM\\*\\* \\*\\*\\_\\_\\*\\*\n",
        "5. \"I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**\"  (1 point)\n",
        "6. Late coins used this pset: \\*\\*\\_\\_\\*\\* Late coins left after submission: \\*\\*\\_\\_\\*\\*\n",
        "7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, \n",
        "    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. \n",
        "8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.\n",
        "9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.\n",
        "10. (Partner 1): tag your submission in Gradescope\n",
        "\n",
        "\\newpage\n"
      ],
      "id": "490d161c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "import time\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "alt.renderers.enable(\"png\")"
      ],
      "id": "95a65ba7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Develop initial scraper and crawler\n",
        "\n",
        "### 1. Scraping (PARTNER 1)\n"
      ],
      "id": "aede8502"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL of the HHS OIG Enforcement Actions page\n",
        "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "\n",
        "# Send a GET request to the page\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()\n",
        "\n",
        "# Parse the HTML with BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Lists to store the extracted data\n",
        "titles = []\n",
        "dates = []\n",
        "categories = []\n",
        "links = []\n",
        "\n",
        "# Find all enforcement action items\n",
        "for item in soup.select(\"li.usa-card\"):\n",
        "    # Extract the title\n",
        "    title_element = item.select_one(\"h2.usa-card__heading a\")\n",
        "    if title_element:\n",
        "        titles.append(title_element.get_text(strip=True))\n",
        "        links.append(\"https://oig.hhs.gov\" + title_element[\"href\"])\n",
        "    else:\n",
        "        titles.append(None)\n",
        "        links.append(None)\n",
        "    \n",
        "    # Extract the date\n",
        "    date_element = item.select_one(\"span.text-base-dark\")\n",
        "    dates.append(date_element.get_text(strip=True) if date_element else None)\n",
        "    \n",
        "    # Extract the category\n",
        "    category_element = item.select_one(\"ul li.usa-tag\")\n",
        "    categories.append(category_element.get_text(strip=True) if category_element else None)\n",
        "\n",
        "# Create a DataFrame with the extracted data\n",
        "data = pd.DataFrame({\n",
        "    \"Title\": titles,\n",
        "    \"Date\": dates,\n",
        "    \"Category\": categories,\n",
        "    \"Link\": links\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "data.to_csv(\"hhs_oig_enforcement_actions.csv\", index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(data.head())"
      ],
      "id": "ba5bfb95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Crawling (PARTNER 1)\n"
      ],
      "id": "9961069d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time \n",
        "\n",
        "# Initialize an empty list to store agency names\n",
        "agencies = []\n",
        "\n",
        "# Loop through each link in the DataFrame to extract agency information\n",
        "for link in data[\"Link\"]:\n",
        "    action_response = requests.get(link)\n",
        "    action_response.raise_for_status()\n",
        "    action_soup = BeautifulSoup(action_response.text, \"html.parser\")\n",
        "    \n",
        "    # Extract agency information - usually the second <li> element\n",
        "    agency_element = action_soup.select(\"ul.usa-list--unstyled li\")\n",
        "    if len(agency_element) > 1:\n",
        "        agency_text = agency_element[1].get_text(strip=True)\n",
        "        agency_name = agency_text.split(\":\", 1)[-1].strip()  # Extract text after \"Agency:\"\n",
        "        agencies.append(agency_name)\n",
        "    else:\n",
        "        agencies.append(None)\n",
        "\n",
        "    # Optional: Pause to avoid too many rapid requests\n",
        "    time.sleep(1)\n",
        "\n",
        "# Update the DataFrame with the new 'Agency' column\n",
        "data[\"Agency\"] = agencies\n",
        "\n",
        "# Print the first few rows to verify\n",
        "print(data.head())"
      ],
      "id": "1e7a7172",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Making the scraper dynamic\n",
        "\n",
        "### 1. Turning the scraper into a function \n",
        "\n",
        "* a. Pseudo-Code (PARTNER 2)\n",
        "\n",
        "\n",
        "1. **Define the Function** `scrape_enforcement_actions(start_month, start_year)`:\n",
        "   - **Inputs**: \n",
        "     - `start_month`: The month to start scraping from (1 to 12).\n",
        "     - `start_year`: The year to start scraping from (should be >= 2013).\n",
        "\n",
        "2. **Initial Validity Check**:\n",
        "   - If `start_year` is less than 2013, **print a message**: “Please enter a year >= 2013, as data before 2013 is not available.” \n",
        "   - **Exit the function** if the condition is not met.\n",
        "\n",
        "3. **Setup**:\n",
        "   - Define the **base URL** for the HHS OIG enforcement actions page.\n",
        "   - **Initialize lists** to store scraped data for each action (e.g., `titles`, `dates`, `categories`, `links`).\n",
        "   - Set `page = 1` to start from the first page of the site.\n",
        "   - Get the **current date** to compare against as the end condition.\n",
        "\n",
        "4. **Loop Through Pages Until Date Condition is Met**:\n",
        "   - Start a `while` loop:\n",
        "     - **Construct URL** with the current `page` number.\n",
        "     - **Request the page** content and parse it with `BeautifulSoup`.\n",
        "     - **Find all enforcement action items** on the page (e.g., by selecting specific HTML elements).\n",
        "\n",
        "     - **Check for End of Pages**:\n",
        "       - If no items are found (end of content), **break** the loop.\n",
        "\n",
        "5. **Extract Data for Each Action**:\n",
        "   - For each item on the page:\n",
        "     - Extract and **store the title**, **link**, **date**, and **category**.\n",
        "     - **Parse the date** to check if it’s within the specified `start_year` and `start_month`.\n",
        "       - If the action’s date is earlier than the specified date, **break** the loop to stop scraping.\n",
        "\n",
        "6. **Append Data to Lists**:\n",
        "   - Append the extracted data for title, date, category, and link to the respective lists.\n",
        "\n",
        "7. **Move to the Next Page**:\n",
        "   - Increment `page` by 1 to load the next set of actions.\n",
        "   - Use `time.sleep(1)` to pause between requests to avoid overloading the server.\n",
        "\n",
        "8. **Create a DataFrame and Save to CSV**:\n",
        "   - After exiting the loop, **create a DataFrame** from the lists.\n",
        "   - **Save** the DataFrame to a CSV file with a name format like `enforcement_actions_<year>_<month>.csv`.\n",
        "\n",
        "9. **End Function**:\n",
        "   - Print a confirmation message that data has been saved.\n",
        "\n",
        "\n",
        "This structure uses a `while` loop to handle pagination dynamically and ensures the scraper stops early if it encounters data before the specified start date. \n",
        "\n",
        "* b. Create Dynamic Scraper (PARTNER 2)\n"
      ],
      "id": "08b292de"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def scrape_enforcement_actions(start_month, start_year):\n",
        "    \"\"\"\n",
        "    Scrapes HHS OIG enforcement actions, including title, date, category, link, and agency name.\n",
        "    Parameters:\n",
        "        start_month (int): The starting month (1-12).\n",
        "        start_year (int): The starting year (must be >= 2013).\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with columns: Title, Date, Category, Link, and Agency.\n",
        "    \"\"\"\n",
        "    # Check if the input year is valid (>= 2013)\n",
        "    if start_year < 2013:\n",
        "        print(\"Please enter a year >= 2013, as data before 2013 is not available.\")\n",
        "        return\n",
        "\n",
        "    # Base URL of the HHS OIG Enforcement Actions page\n",
        "    base_url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "    all_titles, all_dates, all_categories, all_links, all_agencies = [], [], [], [], []\n",
        "    \n",
        "    # Send a GET request to the main page\n",
        "    response = requests.get(base_url)\n",
        "    response.raise_for_status()\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Extract main page data (Title, Date, Category, Link)\n",
        "    for item in soup.select(\"li.usa-card\"):\n",
        "        # Extract title and link\n",
        "        title_element = item.select_one(\"h2.usa-card__heading a\")\n",
        "        title = title_element.get_text(strip=True) if title_element else None\n",
        "        link = \"https://oig.hhs.gov\" + title_element[\"href\"] if title_element else None\n",
        "        all_titles.append(title)\n",
        "        all_links.append(link)\n",
        "\n",
        "        # Extract date\n",
        "        date_element = item.select_one(\"span.text-base-dark\")\n",
        "        date_text = date_element.get_text(strip=True) if date_element else None\n",
        "        all_dates.append(date_text)\n",
        "\n",
        "        # Extract category\n",
        "        category_element = item.select_one(\"ul li.usa-tag\")\n",
        "        category = category_element.get_text(strip=True) if category_element else None\n",
        "        all_categories.append(category)\n",
        "\n",
        "    # Loop through each link to extract the agency information\n",
        "    for link in all_links:\n",
        "        if link:\n",
        "            action_response = requests.get(link)\n",
        "            action_response.raise_for_status()\n",
        "            action_soup = BeautifulSoup(action_response.text, \"html.parser\")\n",
        "\n",
        "            # Extract agency information\n",
        "            agency_element = action_soup.select(\"ul.usa-list--unstyled li\")\n",
        "            if len(agency_element) > 1:\n",
        "                agency_text = agency_element[1].get_text(strip=True)\n",
        "                agency_name = agency_text.split(\":\", 1)[-1].strip()  # Extract text after \"Agency:\"\n",
        "                all_agencies.append(agency_name)\n",
        "            else:\n",
        "                all_agencies.append(None)\n",
        "            \n",
        "            # Optional: Pause to avoid too many rapid requests\n",
        "            time.sleep(1)\n",
        "        else:\n",
        "            all_agencies.append(None)\n",
        "\n",
        "    # Create a DataFrame with the extracted data\n",
        "    data = pd.DataFrame({\n",
        "        \"Title\": all_titles,\n",
        "        \"Date\": all_dates,\n",
        "        \"Category\": all_categories,\n",
        "        \"Link\": all_links,\n",
        "        \"Agency\": all_agencies\n",
        "    })\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    filename = f\"enforcement_actions_{start_year}_{start_month}.csv\"\n",
        "    data.to_csv(filename, index=False)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage:\n",
        "data = scrape_enforcement_actions(1, 2023)\n",
        "print(data.head())\n",
        "\n",
        "# Display the number of enforcement actions collected\n",
        "\n",
        "print(f\"Total enforcement actions collected: {len(data)}\")\n",
        "\n",
        "data[\"Date\"] = pd.to_datetime(data[\"Date\"], errors=\"coerce\")\n",
        "earliest_action = data.sort_values(\"Date\").iloc[0]\n",
        "print(\"Earliest enforcement action collected:\")\n",
        "print(earliest_action)"
      ],
      "id": "d5b78e7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* c. Test Partner's Code (PARTNER 1)\n"
      ],
      "id": "e2471c04"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Plot data based on scraped data\n",
        "\n",
        "### 1. Plot the number of enforcement actions over time (PARTNER 2)\n"
      ],
      "id": "17bdc080"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import altair as alt\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"enforcement_actions_2021_1.csv\")\n",
        "\n",
        "# Convert 'Date' column to datetime format to enable time-based operations\n",
        "data[\"Date\"] = pd.to_datetime(data[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# Drop rows with NaT in Date (if any) after conversion\n",
        "data = data.dropna(subset=[\"Date\"])\n",
        "\n",
        "# Extract month-year for aggregation\n",
        "data[\"Year_Month\"] = data[\"Date\"].dt.to_period(\"M\")\n",
        "\n",
        "# Group by month-year and count the number of actions\n",
        "monthly_counts = data.groupby(\"Year_Month\").size().reset_index(name=\"Count\")\n",
        "\n",
        "# Convert Year_Month to datetime for Altair plotting\n",
        "monthly_counts[\"Year_Month\"] = monthly_counts[\"Year_Month\"].dt.to_timestamp()\n",
        "\n",
        "# Plot the line chart using Altair\n",
        "chart = alt.Chart(monthly_counts).mark_line().encode(\n",
        "    x=alt.X(\"Year_Month:T\", title=\"Month and Year\"),\n",
        "    y=alt.Y(\"Count:Q\", title=\"Number of Enforcement Actions\"),\n",
        "    tooltip=[\"Year_Month:T\", \"Count:Q\"]\n",
        ").properties(\n",
        "    title=\"Number of Enforcement Actions Over Time (Aggregated by Month-Year)\",\n",
        "    width=600,\n",
        "    height=400\n",
        ").interactive()\n",
        "\n",
        "chart"
      ],
      "id": "af803073",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Plot the number of enforcement actions categorized: (PARTNER 1)\n",
        "\n",
        "* based on \"Criminal and Civil Actions\" vs. \"State Enforcement Agencies\"\n"
      ],
      "id": "c7e9b310"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* based on five topics\n"
      ],
      "id": "26421871"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create maps of enforcement activity\n",
        "\n",
        "### 1. Map by State (PARTNER 1)\n"
      ],
      "id": "baa408d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Map by District (PARTNER 2)\n"
      ],
      "id": "cf6a9aa6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# Load the enforcement actions data\n",
        "data = pd.read_csv(\"enforcement_actions_2021_1.csv\")\n",
        "\n",
        "# Filter for US Attorney District-level agencies by looking for \"District\" in the Agency column\n",
        "district_data = data[data[\"Agency\"].str.contains(\"District\", na=False)].copy()\n",
        "\n",
        "# Keep only the last three words in the District column of enforcement data\n",
        "district_data[\"District\"] = district_data[\"Agency\"].apply(lambda x: ' '.join(x.split()[-3:]))\n",
        "\n",
        "# Count the number of enforcement actions per district\n",
        "district_counts = district_data[\"District\"].value_counts().reset_index()\n",
        "district_counts.columns = [\"District\", \"Count\"]\n",
        "\n",
        "# Update the path to the .shp file with either double backslashes or a raw string\n",
        "shapefile_path = r'US Attorney Districts Shapefile simplified_20241105\\geo_export_863694bc-2e85-424f-96a2-1598bf583e4c.shp'\n",
        "\n",
        "# Load the shapefile using GeoPandas\n",
        "district_shapefile = gpd.read_file(shapefile_path)\n",
        "\n",
        "# Remove the first word from each entry in the judicial_d column\n",
        "district_shapefile[\"cleaned_judicial_d\"] = district_shapefile[\"judicial_d\"].apply(lambda x: ' '.join(x.split()[1:]))\n",
        "\n",
        "# Merge the district counts with the shapefile data on 'cleaned_judicial_d' and 'District'\n",
        "district_map_data = district_shapefile.merge(district_counts, left_on=\"cleaned_judicial_d\", right_on=\"District\", how=\"left\")\n",
        "\n",
        "# Fill NaN values in the 'Count' column with 0 for districts with no actions\n",
        "district_map_data[\"Count\"].fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "# Plot the choropleth map with focus on continental U.S.\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "district_map_data.plot(column=\"Count\", cmap=\"Reds\", linewidth=0.8, ax=ax, edgecolor=\"0.8\", legend=True,\n",
        "                       legend_kwds={\"label\": \"Number of Enforcement Actions\",\n",
        "                                    \"orientation\": \"horizontal\"})\n",
        "\n",
        "# Set axis limits to focus on the continental U.S.\n",
        "ax.set_xlim([-130, -60])  # Longitude range for continental U.S.\n",
        "ax.set_ylim([20, 55])     # Latitude range for continental U.S.\n",
        "\n",
        "ax.set_title(\"Number of Enforcement Actions by US Attorney Districts\", fontsize=15)\n",
        "plt.show()"
      ],
      "id": "bf7b6e86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extra Credit\n",
        "\n",
        "### 1. Merge zip code shapefile with population"
      ],
      "id": "fdc95998"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Conduct spatial join"
      ],
      "id": "12b777d7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Map the action ratio in each district"
      ],
      "id": "e0b48694"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\aosil\\AppData\\Local\\Programs\\Python\\Python312\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}